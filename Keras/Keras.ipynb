{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing Handwritten Digits\n",
    "\n",
    "For this goal, we'll use the MNIST (refer to http://yann.lecun.com/exdb/mnist/), a database of handwritten digits made up of a training set of 60,000 examples and a test set of 10,000 examples. Each MNIST image is in greyscale and it consists of 28x28 pixels.\n",
    "\n",
    "Keras provides suitable libraries to load the dataset and split it into training sets and tests sets, used for assessing the performance. Data is converted to `float32` for supporting GPU computation and normalized to `[0, 1]`. In addition, we load the true labels `Y_train` and `Y_test` respectively and perform a one-hot encoding on them.\n",
    "\n",
    "* The input layer has a neuron associated with each pixel in the image for a total of 28 x 28 = 784 neurons, one for each pixel in the MNIST images;\n",
    "* Typically, the values associated with each pixel are normalized in the range [0, 1] (which means that the intensity of each pixel is divided by 255, the maximum intensity value);\n",
    "* The final layer is a single neuron with activation function `softmax`, which is a generalization of the `sigmoid` function;\n",
    "\n",
    "Once we defined the model, we have to compile it so that it can be executed by the Keras backend (either Theano or TensorFlow). There are a few choices to be made during compilation:\n",
    "\n",
    "* We need to select the `optimizer` that is the algorithm used to update weights while we train our model;\n",
    "* We need to select the `objective function` that is used by the optimizer to navigate the space of weights (frequently, objective functions are called `loss function`, and the process of optimization is defined as a process of loss minimization);\n",
    "* We need to evaluate the trained model.\n",
    "\n",
    "Some common choices for metrics (a complete list of Keras metrics is at https://keras.io/metrics/) are as follows:\n",
    "\n",
    "* **Accuracy**: This is the proportion of correct predictions with respect to the targets;\n",
    "* **Precision**: This denotes how many selected items are relevant for a multilabel classification;\n",
    "* **Recal**: This denotes how many selected items are relevant for a multilabel classification.\n",
    "\n",
    "Metrics are similar to objective functions, with the only difference that they are not used for training a model but only for evaluating a model.\n",
    "\n",
    "Once the model is compiled, it can be then trained with the fit() function, which specifies a few parameters:\n",
    "\n",
    "* **epochs**: This is the number of times the model is exposed to the training set. At each iteration, the optimizer tries to adjust the weights so that the objective function is minimized;\n",
    "* **batch_size**: This is the number of training instances observed before the optimizer performs a weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much training data is reserved for validation\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Creates the model\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Selects the optimizer and the evaluation metrics.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE,\n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluates the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "* The network is trained on 48,000 samples, and 12,000 are reserved for validation;\n",
    "* Once the neural model is built, it is then tested on 10,000 samples;\n",
    "* we can notice that the program runs for 200 iterations, and each time, the accuracy improves;\n",
    "\n",
    "This means that a bit less than one handwritten character out of ten is not correctly recognized. We can certainly do better than that.\n",
    "\n",
    "### Improving our neural network\n",
    "\n",
    "* A first improvement is to add additional layers to our network;\n",
    "* So, after the input layer, we have a first dense layer with the `N_HIDDEN` neurons and an activation function `relu`;\n",
    "* This layer is called _hidden_ because it is not directly connected to either the input of the output;\n",
    "* After the first hidden layer, we have a second hidden layer, again with the `N_HIDDEN` neurons, followed by an output layer with 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much training data is reserved for validation\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Selects the optimizer and the evaluation metrics.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluates the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further improving our neural network\n",
    "\n",
    "* The second improvement is to randomly drop with the dropout probability some of the values propagated inside our internal dense network of hidden layers;\n",
    "* In Machine Learning, this is a well known form of regularization;\n",
    "* It has been frequently observed that networks with random dropout in internal hidden layers can generalize better on unseen examples contained in test sets;\n",
    "* One can think of this as each neuron becoming more capable because it knows it cannot depend on its neighbors;\n",
    "* During testing, there is no dropout, so we are now using all our highly tuned neurons;\n",
    "* It is generally a good approach to test how a net performs when some dropout function is adopted.\n",
    "\n",
    "**OBS:** try first training the network with `NB_EPOCH` set to 20. Note that training accuracy should be above test accuracy, otherwise we're not training long enough. After testing it with 20, set the `NB_EPOCH` value to 250 and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Network and training\n",
    "NB_EPOCH = 250\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much training data is reserved for validation\n",
    "DROPOUT = 0.3\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Selects the optimizer and the evaluation metrics.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluates the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different optimizers\n",
    "\n",
    "* Let's focus on one popular training technique known as gradient descent (GD);\n",
    "* The gradient descent can be seen as a hiker who aims at climbing down a mountain into a valley;\n",
    "* Imagine a generic cost function `C(w)` in one single variable `w`;\n",
    "* At each step `r`, the gradient is the direction of maximum increase;\n",
    "* At each step, the hiker can decide what the leg length is before the next step, which is the `learning rate` in gradient descent jargon;\n",
    "* If the learning rate is too small, the hiker will move slowly, but it's too high, the hiker will possibly miss the valley;\n",
    "* In practice, we just choose the activation function, and Keras uses its backend (Tensorflow or Theano) for computing its derivative on our behalf;\n",
    "* When we discuss backpropagation, we will discover that the minimization game is a bit more complex than our toy example;\n",
    "* Keras implements a fast variant of gradient descent known as stochastic gradient descent (`SGD`) and two more advanced optimization techniques known as `RMSprop` and `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "OPTIMIZER = Adam()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much training data is reserved for validation\n",
    "DROPOUT = 0.3\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Selects the optimizer and the evaluation metrics.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluates the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So far, we made progressive improvements; however, the gains are now more and more difficult;\n",
    "* Note that we are optimizing with a dropout of 30%;\n",
    "* For the sake of completeness, it could be useful to report the accuracy on the test only for other dropout values with `Adam` chosen as optimizer.\n",
    "\n",
    "### Increasing the number of epochs\n",
    "\n",
    "* We can make another attempt and increase the number of epochs used for training from 20 to 200;\n",
    "* Unfortunately, this choice increases our computation time by 10, but it gives us no gain;\n",
    "* **Learning is more about adopting smart techniques and not necessarily about the time spent in computations.**\n",
    "\n",
    "### Controlling the optimizer learning rate\n",
    "\n",
    "* There is another attempt we can make, which is changing the learning parameter for our optimizer;\n",
    "* If you plot different values, you'll see that the optimal value is somewhere close to 0.001.\n",
    "\n",
    "### Increasing the number of internal hidden neurons\n",
    "\n",
    "* We can make yet another attempt, that is, changing the number of internal hidden neurons;\n",
    "* We report the results of the experiments with an increasing number of hidden neurons;\n",
    "* By increasing the complexity of the model, the run time increases significantly because there are more and more parameters to optimize.\n",
    "\n",
    "### Increasing the size of batch computation\n",
    "\n",
    "* Gradient descent tries to minimize the cost function on all the examples provided in the training sets;\n",
    "* Stochastic gradient descent considers only `BATCH_SIZE`;\n",
    "* If we check the behavior is by changing this parameter, we notice that the optimal accuracy value is reached for BATCH_SIZE=128.\n",
    "\n",
    "### Adopting regularization for avoiding overfitting\n",
    "\n",
    "* A model can become excessively complex in order to capture all the relations inherently expressed by the training data, which can bring two problems:\n",
    "  - First, a complex model might require a significant amount of time to be executed;\n",
    "  - Second, a complex model can achieve good performance on training data and not be able to generalize on unsee data.\n",
    "* As a rule of thumb, if during the training we see that the loss increases on validation, after an initial decrease, then we have a problem of model complexity that overfits training;\n",
    "* In order to solve the overfitting problem, we need a way to capture the complexity of a model, that is, how complex a model can be;\n",
    "* A model is nothing more than a vector of weights. Therefore the complexity of a model can be conveniently represented as the number of nonzero weights;\n",
    "* If we have two models, M1 and M2, achieving pretty much the same performance in terms of loss function, then we should choose the simplest model that has the minimum number of nonzero weights;\n",
    "* Playing with regularization can be a good way to increase the performance of a network, in particular when there is an evident situation of overfitting.\n",
    "* There 3 types of regularization in machine learning:\n",
    "  - **L1 regularization** (also known as **lasso**): The complexity of the model is expressed as the sum of the absolute values of the weights;\n",
    "  - **L2 regularization** (also known as **ridge**): The complexity of the model is expressed as the sum of the squares of the weights;\n",
    "  - **Elastic net regularization**: The complexity of the model is captured by a combination of the two preceding techniques.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "* For a given net, there are indeed multiple parameters that can be optimized (such as the number of hidden neurons, BATCH_SIZE, number of epochs, and many more);\n",
    "* Hyperparameter tuning is the process of finding the optimal combination of those parameters that minimize cost functions.\n",
    "* In other words, the parameters are divided into buckets, and different combinations of values are checked via a brute force approach.\n",
    "\n",
    "### Predicting Output\n",
    "\n",
    "* You can use the following method for predicting the output with Keras:\n",
    "* `model.predict(X)`: This is used to predict the Y values;\n",
    "* `model.evaluate()`: This is used to compute the loss values;\n",
    "* `model.predict_classes()`: This is used to compute category outputs;\n",
    "* `model.predict_proba()`: This is used to compute class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Keras\n",
    "\n",
    "### What is a tensor?\n",
    "\n",
    "* A tensor is nothing but a multidimensional array or matrix;\n",
    "* Keras uses either Theano or TensorFlow to perform very efficient computations on tensors;\n",
    "* Both the backends are capable of efficient symbolic computations on tensors, which are the fundamental building blocks for creating neural networks.\n",
    "\n",
    "### Predefined Neural Network Layers\n",
    "\n",
    "* **Regular dense**: A dense model is a fully connected neural network layer;\n",
    "* **Recurrent neural networks -- simple LSTM and GRU**: Recurrent neural networks are a class of neural networks that exploit the sequential nature or their input. Such inputs could be a text, a speech, time series, and anything else where the occurrence of an element in the sequence is dependent on the elements that appeared before it;\n",
    "* **Convolutional and pooling layers**: ConvNets are a class of neural networks using convolutional and pooling operations for progressively learning rather sophisticated models based on progressive levels of abstraction. It resembles vision models that have evolved over millions of years inside the human brain. People called it deep with 3-5 layers a few years ago, and now it has gone up to 100-200;\n",
    "* **Regularization**: Regularization is a way to prevent overfitting. Multiple layers have parameters for regularization. One example is `Dropout`, but there are others;\n",
    "* **Batch normalization**: It's a way to accelerate learning and generally achieve better accuracy;\n",
    "\n",
    "### Losses functions\n",
    "\n",
    "Losses functions (or objective functions, or optimization score function) can be classified into four categories:\n",
    "\n",
    "* **Accuracy** which is used for classification problems;\n",
    "* **Error loss**, which measures the difference between the values predicted and the values actually observed. There are multiple choices: `mse` (mean square error), `rmse` (root mean square error), `mae` (mean absolute error), `mape` (mean percentage error) and `msle` (mean squared logarithmic error);\n",
    "* **Hinge loss**, which is generally used for training classifiers;\n",
    "* **Class loss** is used to calculate the cross-entropy for classification problems (see https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "### Metrics\n",
    "\n",
    "A metric function is similar to an objective function. The only difference is that the results from evaluating a metric are not used when training the model.\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "Optimizers include `SGD`, `RMSprop`, and `Adam`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning with Convolutional Networks (ConvNets)\n",
    "\n",
    "* Leverage spacial information and are suited for classifying images;\n",
    "* Based on how our vision is based on multiple cortex levels, with each one recognizing more and more structured information;\n",
    "* Two different types of layers, convolutional and pooling, are typically alternated.\n",
    "\n",
    "### Local receptive fields\n",
    "\n",
    "* To preserve spatial information, we represent each image with a matrix of pixels;\n",
    "* A simple way to encode the local structure is to connect a submatrix of adjacent input neurons into one single hidden neuron (which is the **local receptive field**) belonging to the next layer;\n",
    "* Of course, we can encode more information by having overlapping submatrices;\n",
    "* In Keras, the size of each single submatrix is called _stride length_, and this is a hyperparameter that can be fine-tuned during the construction of our nets;\n",
    "* Of course, we can have multiple feature maps that learn independently from each hidden layer.\n",
    "\n",
    "![ConvNet example](ConvNet.gif)\n",
    "\n",
    "* Rather than focus on one pixel at a time, ConvNets take in square patches of pixels and passes them through a _filter_ (or _kernel_), and the job of the filter is to find patterns in the pixels;\n",
    "* We are going to take the dot product of the filter with this patch of the image channel. If the two matrices have high values in the same positions, the dot product’s output will be high. If they don’t, it will be low.\n",
    "* We start in the upper lefthand corner of the image and we move the filter across the image step by step until it reaches the upper righthand corner. The size of the step is known as `stride`. You can move the filter to the right 1 column at a time, or you can choose to make larger steps;\n",
    "* At each step, you take another dot product, and you place the results of that dot product in a third matrix known as an `activation map`;\n",
    "* The width, or number of columns, of the activation map is equal to the number of steps the filter takes to traverse the underlying image;\n",
    "* Since larger strides lead to fewer steps, a big stride will produce a smaller activation map.\n",
    "* This is important, because the size of the matrices that convolutional networks process and produce at each layer is directly proportional to how computationally expensive they are and how much time they take to train.\n",
    "* **A larger stride means less time and compute.**\n",
    "\n",
    "### Max Pooling/Downsampling\n",
    "\n",
    "![Max Pool example](MaxPool.png)\n",
    "\n",
    "* The activation maps are fed into a downsampling layer, and like convolutions, this method is applied one patch at a time;\n",
    "* In this case, max pooling simply takes the largest value from one patch of an image;\n",
    "* Much information is lost in this step, which has spurred research into alternative methods. But downsampling has the advantage, precisely because information is lost, of decreasing the amount of storage and processing required;\n",
    "\n",
    "### Average Pooling\n",
    "\n",
    "* The alternative method to Max Pooling is simply taking the average of the regions, which is called _average pooling_.\n",
    "\n",
    "### ConvNets Summary\n",
    "\n",
    "![ConvNets Summary](ConvNetSummary.png)\n",
    "\n",
    "In the image above you can see:\n",
    "\n",
    "* The actual input image that is scanned for features;\n",
    "* Activation maps stacked atop one another, one for each filter you employ;\n",
    "* The activation maps condensed through downsampling;\n",
    "* A new set of activation maps created by passing filters over the first downsampled stack;\n",
    "* The second downsampling, which condenses the second set of activation maps;\n",
    "* A fully connected layer that classifies output with one label per node.\n",
    "\n",
    "There are various architectures of CNNs available which have been key in building algorithms which power and shall power AI as a whole in the foreseeable future:\n",
    "\n",
    "1. LeNet\n",
    "2. AlexNet\n",
    "3. VGGNet\n",
    "4. GoogLeNet\n",
    "5. ResNet\n",
    "6. ZFNet\n",
    "\n",
    "### LeNet code in Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#define the ConvNet\n",
    "class LeNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes):\n",
    "        model = Sequential()\n",
    "        # CONV => RELU => POOL\n",
    "        # Here, 20 is the number of convolution kernels/filters to use, each one with the size 5x5\n",
    "        # padding='same' means that padding is used\n",
    "        # Output dimension is the same one of the input shape, so it will be 28 x 28\n",
    "        # pool_size=(2, 2) represents the factors by which the image is vertically and horizontally downscaled\n",
    "        model.add(Conv2D(20, kernel_size=5, padding=\"same\", input_shape=input_shape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # CONV => RELU => POOL\n",
    "        # A second convolutional stage with ReLU activations follows\n",
    "        # In this case, we increase the number of convolutional filters learned to 50\n",
    "        # Increasing the number of filters in deeper layers is a common technique used in deep learning\n",
    "        model.add(Conv2D(50, kernel_size=5, border_mode=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # Flatten => RELU layers\n",
    "        # Pretty standard flattening and a dense network of 500 neurons\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        # Softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model\n",
    "\n",
    "# Training parameters\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "OPTIMIZER = Adam()\n",
    "VALIDATION_SPLIT = 0.2\n",
    "IMG_ROWS, IMG_COLS = 28, 28 # input image dimensions\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "INPUT_SHAPE = (1, IMG_ROWS, IMG_COLS)\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "K.set_image_dim_ordering(\"th\")\n",
    "\n",
    "# consider them as float and normalize\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# we need a 60K x [1 x 28 x 28] shape as input to the CONVNET\n",
    "X_train = X_train[:, np.newaxis, :, :]\n",
    "X_test = X_test[:, np.newaxis, :, :]\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# initialize the optimizer and model\n",
    "model = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
