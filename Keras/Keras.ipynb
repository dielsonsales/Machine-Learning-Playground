{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing Handwritten Digits\n",
    "\n",
    "For this goal, we'll use the MNIST (refer to http://yann.lecun.com/exdb/mnist/), a database of handwritten digits made up of a training set of 60,000 examples and a test set of 10,000 examples. Each MNIST image is in greyscale and it consists of 28x28 pixels.\n",
    "\n",
    "Keras provides suitable libraries to load the dataset and split it into training sets and tests sets, used for assessing the performance. Data is converted to `float32` for supporting GPU computation and normalized to `[0, 1]`. In addition, we load the true labels `Y_train` and `Y_test` respectively and perform a one-hot encoding on them.\n",
    "\n",
    "* The input layer has a neuron associated with each pixel in the image for a total of 28 x 28 = 784 neurons, one for each pixel in the MNIST images;\n",
    "* Typically, the values associated with each pixel are normalized in the range [0, 1] (which means that the intensity of each pixel is divided by 255, the maximum intensity value);\n",
    "* The final layer is a single neuron with activation function `softmax`, which is a generalization of the `sigmoid` function;\n",
    "\n",
    "Once we defined the model, we have to compile it so that it can be executed by the Keras backend (either Theano or TensorFlow). There are a few choices to be made during compilation:\n",
    "\n",
    "* We need to select the `optimizer` that is the algorithm used to update weights while we train our model;\n",
    "* We need to select the `objective function` that is used by the optimizer to navigate the space of weights (frequently, objective functions are called `loss function`, and the process of optimization is defined as a process of loss minimization);\n",
    "* We need to evaluate the trained model.\n",
    "\n",
    "Some common choices for metrics (a complete list of Keras metrics is at https://keras.io/metrics/) are as follows:\n",
    "\n",
    "* **Accuracy**: This is the proportion of correct predictions with respect to the targets;\n",
    "* **Precision**: This denotes how many selected items are relevant for a multilabel classification;\n",
    "* **Recal**: This denotes how many selected items are relevant for a multilabel classification.\n",
    "\n",
    "Metrics are similar to objective functions, with the only difference that they are not used for training a model but only for evaluating a model.\n",
    "\n",
    "Once the model is compiled, it can be then trained with the fit() function, which specifies a few parameters:\n",
    "\n",
    "* **epochs**: This is the number of times the model is exposed to the training set. At each iteration, the optimizer tries to adjust the weights so that the objective function is minimized;\n",
    "* **batch_size**: This is the number of training instances observed before the optimizer performs a weight update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much training data is reserved for validation\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Creates the model\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Selects the optimizer and the evaluation metrics.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE,\n",
    "                    validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluates the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "* The network is trained on 48,000 samples, and 12,000 are reserved for validation;\n",
    "* Once the neural model is built, it is then tested on 10,000 samples;\n",
    "* we can notice that the program runs for 200 iterations, and each time, the accuracy improves;\n",
    "\n",
    "This means that a bit less than one handwritten character out of ten is not correctly recognized. We can certainly do better than that.\n",
    "\n",
    "### Improving our neural network\n",
    "\n",
    "* A first improvement is to add additional layers to our network;\n",
    "* So, after the input layer, we have a first dense layer with the `N_HIDDEN` neurons and an activation function `relu`;\n",
    "* This layer is called _hidden_ because it is not directly connected to either the input of the output;\n",
    "* After the first hidden layer, we have a second hidden layer, again with the `N_HIDDEN` neurons, followed by an output layer with 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much training data is reserved for validation\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Selects the optimizer and the evaluation metrics.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluates the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further improving our neural network\n",
    "\n",
    "* The second improvement is to randomly drop with the dropout probability some of the values propagated inside our internal dense network of hidden layers;\n",
    "* In Machine Learning, this is a well known form of regularization;\n",
    "* It has been frequently observed that networks with random dropout in internal hidden layers can generalize better on unseen examples contained in test sets;\n",
    "* One can think of this as each neuron becoming more capable because it knows it cannot depend on its neighbors;\n",
    "* During testing, there is no dropout, so we are now using all our highly tuned neurons;\n",
    "* It is generally a good approach to test how a net performs when some dropout function is adopted.\n",
    "\n",
    "**OBS:** try first training the network with `NB_EPOCH` set to 20. Note that training accuracy should be above test accuracy, otherwise we're not training long enough. After testing it with 20, set the `NB_EPOCH` value to 250 and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Network and training\n",
    "NB_EPOCH = 250\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "OPTIMIZER = SGD()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much training data is reserved for validation\n",
    "DROPOUT = 0.3\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Selects the optimizer and the evaluation metrics.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluates the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different optimizers\n",
    "\n",
    "* Let's focus on one popular training technique known as gradient descent (GD);\n",
    "* The gradient descent can be seen as a hiker who aims at climbing down a mountain into a valley;\n",
    "* Imagine a generic cost function `C(w)` in one single variable `w`;\n",
    "* At each step `r`, the gradient is the direction of maximum increase;\n",
    "* At each step, the hiker can decide what the leg length is before the next step, which is the `learning rate` in gradient descent jargon;\n",
    "* If the learning rate is too small, the hiker will move slowly, but it's too high, the hiker will possibly miss the valley;\n",
    "* In practice, we just choose the activation function, and Keras uses its backend (Tensorflow or Theano) for computing its derivative on our behalf;\n",
    "* When we discuss backpropagation, we will discover that the minimization game is a bit more complex than our toy example;\n",
    "* Keras implements a fast variant of gradient descent known as stochastic gradient descent (`SGD`) and two more advanced optimization techniques known as `RMSprop` and `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Network and training\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "OPTIMIZER = Adam()\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT = 0.2 # how much training data is reserved for validation\n",
    "DROPOUT = 0.3\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "RESHAPED = 784\n",
    "\n",
    "# X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Normalize\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Selects the optimizer and the evaluation metrics.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "# Trains the model\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "# Evaluates the model\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* So far, we made progressive improvements; however, the gains are now more and more difficult;\n",
    "* Note that we are optimizing with a dropout of 30%;\n",
    "* For the sake of completeness, it could be useful to report the accuracy on the test only for other dropout values with `Adam` chosen as optimizer.\n",
    "\n",
    "### Increasing the number of epochs\n",
    "\n",
    "* We can make another attempt and increase the number of epochs used for training from 20 to 200;\n",
    "* Unfortunately, this choice increases our computation time by 10, but it gives us no gain;\n",
    "* **Learning is more about adopting smart techniques and not necessarily about the time spent in computations.**\n",
    "\n",
    "### Controlling the optimizer learning rate\n",
    "\n",
    "* There is another attempt we can make, which is changing the learning parameter for our optimizer;\n",
    "* If you plot different values, you'll see that the optimal value is somewhere close to 0.001.\n",
    "\n",
    "### Increasing the number of internal hidden neurons\n",
    "\n",
    "* We can make yet another attempt, that is, changing the number of internal hidden neurons;\n",
    "* We report the results of the experiments with an increasing number of hidden neurons;\n",
    "* By increasing the complexity of the model, the run time increases significantly because there are more and more parameters to optimize.\n",
    "\n",
    "### Increasing the size of batch computation\n",
    "\n",
    "* Gradient descent tries to minimize the cost function on all the examples provided in the training sets;\n",
    "* Stochastic gradient descent considers only `BATCH_SIZE`;\n",
    "* If we check the behavior is by changing this parameter, we notice that the optimal accuracy value is reached for BATCH_SIZE=128.\n",
    "\n",
    "### Adopting regularization for avoiding overfitting\n",
    "\n",
    "* A model can become excessively complex in order to capture all the relations inherently expressed by the training data, which can bring two problems:\n",
    "  - First, a complex model might require a significant amount of time to be executed;\n",
    "  - Second, a complex model can achieve good performance on training data and not be able to generalize on unsee data.\n",
    "* As a rule of thumb, if during the training we see that the loss increases on validation, after an initial decrease, then we have a problem of model complexity that overfits training;\n",
    "* In order to solve the overfitting problem, we need a way to capture the complexity of a model, that is, how complex a model can be;\n",
    "* A model is nothing more than a vector of weights. Therefore the complexity of a model can be conveniently represented as the number of nonzero weights;\n",
    "* If we have two models, M1 and M2, achieving pretty much the same performance in terms of loss function, then we should choose the simplest model that has the minimum number of nonzero weights;\n",
    "* Playing with regularization can be a good way to increase the performance of a network, in particular when there is an evident situation of overfitting.\n",
    "* There 3 types of regularization in machine learning:\n",
    "  - **L1 regularization** (also known as **lasso**): The complexity of the model is expressed as the sum of the absolute values of the weights;\n",
    "  - **L2 regularization** (also known as **ridge**): The complexity of the model is expressed as the sum of the squares of the weights;\n",
    "  - **Elastic net regularization**: The complexity of the model is captured by a combination of the two preceding techniques.\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "* For a given net, there are indeed multiple parameters that can be optimized (such as the number of hidden neurons, BATCH_SIZE, number of epochs, and many more);\n",
    "* Hyperparameter tuning is the process of finding the optimal combination of those parameters that minimize cost functions.\n",
    "* In other words, the parameters are divided into buckets, and different combinations of values are checked via a brute force approach.\n",
    "\n",
    "### Predicting Output\n",
    "\n",
    "* You can use the following method for predicting the output with Keras:\n",
    "* `model.predict(X)`: This is used to predict the Y values;\n",
    "* `model.evaluate()`: This is used to compute the loss values;\n",
    "* `model.predict_classes()`: This is used to compute category outputs;\n",
    "* `model.predict_proba()`: This is used to compute class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Keras\n",
    "\n",
    "### What is a tensor?\n",
    "\n",
    "* A tensor is nothing but a multidimensional array or matrix;\n",
    "* Keras uses either Theano or TensorFlow to perform very efficient computations on tensors;\n",
    "* Both the backends are capable of efficient symbolic computations on tensors, which are the fundamental building blocks for creating neural networks.\n",
    "\n",
    "### Predefined Neural Network Layers\n",
    "\n",
    "* **Regular dense**: A dense model is a fully connected neural network layer;\n",
    "* **Recurrent neural networks -- simple LSTM and GRU**: Recurrent neural networks are a class of neural networks that exploit the sequential nature or their input. Such inputs could be a text, a speech, time series, and anything else where the occurrence of an element in the sequence is dependent on the elements that appeared before it;\n",
    "* **Convolutional and pooling layers**: ConvNets are a class of neural networks using convolutional and pooling operations for progressively learning rather sophisticated models based on progressive levels of abstraction. It resembles vision models that have evolved over millions of years inside the human brain. People called it deep with 3-5 layers a few years ago, and now it has gone up to 100-200;\n",
    "* **Regularization**: Regularization is a way to prevent overfitting. Multiple layers have parameters for regularization. One example is `Dropout`, but there are others;\n",
    "* **Batch normalization**: It's a way to accelerate learning and generally achieve better accuracy;\n",
    "\n",
    "### Losses functions\n",
    "\n",
    "Losses functions (or objective functions, or optimization score function) can be classified into four categories:\n",
    "\n",
    "* **Accuracy** which is used for classification problems;\n",
    "* **Error loss**, which measures the difference between the values predicted and the values actually observed. There are multiple choices: `mse` (mean square error), `rmse` (root mean square error), `mae` (mean absolute error), `mape` (mean percentage error) and `msle` (mean squared logarithmic error);\n",
    "* **Hinge loss**, which is generally used for training classifiers;\n",
    "* **Class loss** is used to calculate the cross-entropy for classification problems (see https://en.wikipedia.org/wiki/Cross_entropy).\n",
    "\n",
    "### Metrics\n",
    "\n",
    "A metric function is similar to an objective function. The only difference is that the results from evaluating a metric are not used when training the model.\n",
    "\n",
    "### Optimizers\n",
    "\n",
    "Optimizers include `SGD`, `RMSprop`, and `Adam`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning with Convolutional Networks (ConvNets)\n",
    "\n",
    "* Leverage spacial information and are suited for classifying images;\n",
    "* Based on how our vision is based on multiple cortex levels, with each one recognizing more and more structured information;\n",
    "* Two different types of layers, convolutional and pooling, are typically alternated.\n",
    "\n",
    "### Local receptive fields\n",
    "\n",
    "* To preserve spatial information, we represent each image with a matrix of pixels;\n",
    "* A simple way to encode the local structure is to connect a submatrix of adjacent input neurons into one single hidden neuron (which is the **local receptive field**) belonging to the next layer;\n",
    "* Of course, we can encode more information by having overlapping submatrices;\n",
    "* In Keras, the size of each single submatrix is called _stride length_, and this is a hyperparameter that can be fine-tuned during the construction of our nets;\n",
    "* Of course, we can have multiple feature maps that learn independently from each hidden layer.\n",
    "\n",
    "![ConvNet example](ConvNet.gif)\n",
    "\n",
    "* Rather than focus on one pixel at a time, ConvNets take in square patches of pixels and passes them through a _filter_ (or _kernel_), and the job of the filter is to find patterns in the pixels;\n",
    "* We are going to take the dot product of the filter with this patch of the image channel. If the two matrices have high values in the same positions, the dot product’s output will be high. If they don’t, it will be low.\n",
    "* We start in the upper lefthand corner of the image and we move the filter across the image step by step until it reaches the upper righthand corner. The size of the step is known as `stride`. You can move the filter to the right 1 column at a time, or you can choose to make larger steps;\n",
    "* At each step, you take another dot product, and you place the results of that dot product in a third matrix known as an `activation map`;\n",
    "* The width, or number of columns, of the activation map is equal to the number of steps the filter takes to traverse the underlying image;\n",
    "* Since larger strides lead to fewer steps, a big stride will produce a smaller activation map.\n",
    "* This is important, because the size of the matrices that convolutional networks process and produce at each layer is directly proportional to how computationally expensive they are and how much time they take to train.\n",
    "* **A larger stride means less time and compute.**\n",
    "\n",
    "### Max Pooling/Downsampling\n",
    "\n",
    "![Max Pool example](MaxPool.png)\n",
    "\n",
    "* The activation maps are fed into a downsampling layer, and like convolutions, this method is applied one patch at a time;\n",
    "* In this case, max pooling simply takes the largest value from one patch of an image;\n",
    "* Much information is lost in this step, which has spurred research into alternative methods. But downsampling has the advantage, precisely because information is lost, of decreasing the amount of storage and processing required;\n",
    "\n",
    "### Average Pooling\n",
    "\n",
    "* The alternative method to Max Pooling is simply taking the average of the regions, which is called _average pooling_.\n",
    "\n",
    "### ConvNets Summary\n",
    "\n",
    "![ConvNets Summary](ConvNetSummary.png)\n",
    "\n",
    "In the image above you can see:\n",
    "\n",
    "* The actual input image that is scanned for features;\n",
    "* Activation maps stacked atop one another, one for each filter you employ;\n",
    "* The activation maps condensed through downsampling;\n",
    "* A new set of activation maps created by passing filters over the first downsampled stack;\n",
    "* The second downsampling, which condenses the second set of activation maps;\n",
    "* A fully connected layer that classifies output with one label per node.\n",
    "\n",
    "There are various architectures of CNNs available which have been key in building algorithms which power and shall power AI as a whole in the foreseeable future:\n",
    "\n",
    "1. LeNet\n",
    "2. AlexNet\n",
    "3. VGGNet\n",
    "4. GoogLeNet\n",
    "5. ResNet\n",
    "6. ZFNet\n",
    "\n",
    "### LeNet code in Keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "#define the ConvNet\n",
    "class LeNet:\n",
    "    @staticmethod\n",
    "    def build(input_shape, classes):\n",
    "        model = Sequential()\n",
    "        # CONV => RELU => POOL\n",
    "        # Here, 20 is the number of convolution kernels/filters to use, each one with the size 5x5 and padding='same' means that padding is used.\n",
    "        # Output dimension is the same one of the input shape, so it will be 28 x 28\n",
    "        # pool_size=(2, 2) represents the factors by which the image is vertically and horizontally downscaled\n",
    "        model.add(Conv2D(20, kernel_size=5, padding=\"same\", input_shape=input_shape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # CONV => RELU => POOL\n",
    "        # A second convolutional stage with ReLU activations follows\n",
    "        # In this case, we increase the number of convolutional filters learned to 50\n",
    "        # Increasing the number of filters in deeper layers is a common technique used in deep learning\n",
    "        model.add(Conv2D(50, kernel_size=5, border_mode=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "        # Flatten => RELU layers\n",
    "        # Pretty standard flattening and a dense network of 500 neurons\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(500))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        # Softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        return model\n",
    "\n",
    "# Training parameters\n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "OPTIMIZER = Adam()\n",
    "VALIDATION_SPLIT = 0.2\n",
    "IMG_ROWS, IMG_COLS = 28, 28 # input image dimensions\n",
    "NB_CLASSES = 10 # number of outputs\n",
    "INPUT_SHAPE = (1, IMG_ROWS, IMG_COLS)\n",
    "\n",
    "# data: shuffled and split between train and test sets\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "K.set_image_dim_ordering(\"th\")\n",
    "\n",
    "# consider them as float and normalize\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# we need a 60K x [1 x 28 x 28] shape as input to the CONVNET\n",
    "X_train = X_train[:, np.newaxis, :, :]\n",
    "X_test = X_test[:, np.newaxis, :, :]\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# initialize the optimizer and model\n",
    "model = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=OPTIMIZER, metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Another test that we can run to better understand the power of deep learning and ConvNet is to reduce the size of the training set and observe the consequent decay in performance;\n",
    "* The proper training set used for training our model will progressively reduce its size of (5900, 3000, 1800, 600, and 300) examples;\n",
    "* Our test set is always fixed and it consists of 10,000 examples;\n",
    "* Our deep network always outperforms the simple network and the gap is more and more evident when the number of examples provided for training is progressively reduced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot\n",
    "X      = np.array([5900, 3000, 1800, 600, 300])\n",
    "Y_conv = np.array([96.68, 92.32, 90.00, 79.14, 72.44])\n",
    "Y      = np.array([85.56, 81.76, 76.65, 60.26, 48.26])\n",
    "plt.ylim((0, 100))\n",
    "plt.xlabel('Training samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(X, Y_conv, 'b-', X, Y, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizing CIFAR-10 images with deep learning\n",
    "\n",
    "* The CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in 3 channels divided into 10 classes. Each class contains 6000 images;\n",
    "* The training set contains 50,000 images, while the test set provides 10,000 images;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "#load dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert to categorical\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Our net will learn 32 convolutional filters, each of which with a 3 x 3 size.\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "# Dense lyer of 512 units and ReLU activation + dropout at 50% + softmax layer with 10 classes (one for each category)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Training\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Save the model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the CIFAR-10 performance with a deeper network\n",
    "\n",
    "* One way to improve the performance is to define a deeper network with multiple convolutional operations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "#constant\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "#load dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert to categorical\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# conv + conv + maxpool + dropout + conv + conv + maxpool + dense + dropout + dense\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Training\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=BATCH_SIZE, epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Save the model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the CIFAR-10 performance with data augmentation\n",
    "\n",
    "* Another way to improve the performance is to generate more images for our training;\n",
    "* We can take the CIFAR training set and augment it with multiple transformations including rotation, rescaling, horizontal/vertical flip, zooming, channel shift, and many more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 32\n",
    "IMG_COLS = 32\n",
    "\n",
    "# Constants\n",
    "NUM_TO_AUGMENT=5\n",
    "BATCH_SIZE = 128\n",
    "NB_EPOCH = 20\n",
    "NB_CLASSES = 10\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2\n",
    "OPTIM = RMSprop()\n",
    "\n",
    "#load dataset\n",
    "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
    "\n",
    "# Augumenting\n",
    "# The rotation_range is a value in degrees (0 - 180) for randomly rotating pictures\n",
    "# width_shift and height_shift are ranges for randomly translating pictures vertically or horizontally\n",
    "# zoom_range is for randomly zooming pictures\n",
    "# horizontal_flip is for randomly flipping half of the images horizontally\n",
    "# fill_mode is the strategy used for filling in new pixels that can appear after a rotation or a shift\n",
    "print(\"Augmenting training set images...\")\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Convert to categorical\n",
    "Y_train = np_utils.to_categorical(Y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(Y_test, NB_CLASSES)\n",
    "\n",
    "# Float and normalization\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# conv + conv + maxpool + dropout + conv + conv + maxpool + dense + dropout + dense\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same', input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NB_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Training\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIM, metrics=['accuracy'])\n",
    "history = model.fit_generator(\n",
    "    datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n",
    "    samples_per_epoch=len(X_train),\n",
    "    epochs=NB_EPOCH,\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# Save the model\n",
    "model_json = model.to_json()\n",
    "open('cifar10_architecture.json', 'w').write(model_json)\n",
    "model.save_weights('cifar10_weights.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with CIFAR-10\n",
    "\n",
    "* Suppose that we want to use the deep learning model we just trained for CIFAR-10 for a bulk evaluation of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Load the model\n",
    "model_architecture = 'cifar10_architecture.json'\n",
    "model_weights = 'cifar10_weights.h5'\n",
    "model = model_from_json(open(model_architecture).read())\n",
    "model.load_weights(model_weights)\n",
    "\n",
    "# Load images\n",
    "img_names = ['cat-standing.jpg', 'dog.jpg']\n",
    "imgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name), (32, 32)), (1, 0, 2)).astype('float32') for img_name in img_names]\n",
    "imgs = np.array(imgs) / 255\n",
    "\n",
    "# Train\n",
    "optim = SGD()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "# Predict\n",
    "predictions = model.predict_classes(imgs)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very deep convolutional networks for largescale image recognition\n",
    "\n",
    "* In 2014, an interesting contribution for image recognition was presented:\n",
    "* The paper shows that, a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers;\n",
    "* One model in the paper denoted as D or **VGG-16** has 16 deep layers.\n",
    "\n",
    "### Utilizing Keras built-in VGG-16 net module\n",
    "\n",
    "* Keras _applications_ are pre-built and pre-trained deep learning models;\n",
    "* Weights are downloaded automatically when instantiating a model and stored at `~/.keras/models/`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Prebuild model with pre-trained weights on imagenet\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "\n",
    "# Resize into VGG16 trained images' format\n",
    "im = cv2.resize(cv2.imread('steam-locomotive.png'), (224, 224))\n",
    "im = np.expand_dims(im, axis=0)\n",
    "\n",
    "# Predict\n",
    "out = model.predict(im)\n",
    "plt.plot(out.ravel())\n",
    "plt.show()\n",
    "print(np.argmax(out)) # this should print 820 for steaming train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recycling pre-built deep learning models for extracting features\n",
    "\n",
    "* One very simple idea is to use VGG-16 and, more generally, DCNN, for feature extraction.\n",
    "* Why we want to extract the features from an intermediate layer in a DCNN?\n",
    "  - as the network learns to classify images into categories, each layer learns to identify the features that are necessary to do the final classification;\n",
    "  - Lower layers identify lower order features such as color and edges;\n",
    "  - Higher layers compose these lower order feature into higher order features such as shapes or objects.\n",
    "* This has many advantages:\n",
    "  - We can rely on publicly available large-scale training and transfer this learning to novel domains;\n",
    "  - We can save time for expensive large training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# Pre-built and pre-trained deep learning VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print (i, layer.name, layer.output_shape)\n",
    "\n",
    "# Extract features from block4_pool block\n",
    "model = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)\n",
    "img_path = 'cat.png'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "# Get the features from this block\n",
    "features = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very deep inception-v3 net used for transfer learning\n",
    "\n",
    "* Transfer learning is a very powerful deep learning technique which has more applications in different domains;\n",
    "* It works like using your knowledge of English to learn Spanish;\n",
    "* Computer vision researchers commonly use pre-trained CNNs to generate representations for novel tasks, where the dataset may not be large enough to train an entire CNN from scratch;\n",
    "* Another common tactic is to take the pretrained ImageNet network and then to fine-tune the entire network to the novel task;\n",
    "* **Inception-v3** net is a very deep ConvNet developed by Google;\n",
    "* The default input size for this model is 299 x 299 on three channels;\n",
    "\n",
    "![Inception-v3](Inception-v3.png)\n",
    "\n",
    "* Suppose to have a training dataset D in a domain, different from ImageNet. D has 1,024 features in input and 200 categories in output;\n",
    "* The top level is a dense layer with 1,024 inputs and where the last output level is a softmax dense layer with 200 classes of output;\n",
    "* `x = GlobalAveragePooling2D()(x)` is used to convert the input to the correct shape for the dense layer to handle;\n",
    "\n",
    "The `base_model.output` tensor has the shape `(samples, channels, rows, cols)` for `dim_ordering=\"th\"` or `(samples, rows, cols, channels)` for `dim_ordering=\"tf\"` but dense needs them as `(samples, channels)` and `GlobalAveragePooling2D` averages across `(rows, cols)`. So if you look at the last four layers (where `include_top=True`), you see these shapes:\n",
    "\n",
    "```\n",
    "# layer.name, layer.input_shape, layer.output_shape\n",
    "('mixed10', [(None, 8, 8, 320), (None, 8, 8, 768), (None, 8, 8, 768),\n",
    "(None, 8, 8, 192)], (None, 8, 8, 2048))\n",
    "('avg_pool', (None, 8, 8, 2048), (None, 1, 1, 2048))\n",
    "('flatten', (None, 1, 1, 2048), (None, 2048))\n",
    "('predictions', (None, 2048), (None, 1000))\n",
    "```\n",
    "\n",
    "When you do `include_top=False`, you are removing the last three layers and exposing the `mixed10` layer so the `GlobalAveragePooling2D` layer converts the `(None, 8, 8, 2048)` to `(None, 2048)`, where each element in the `(None, 2048)` tensor is the average value for each corresponding `(8, 8)` subtensor in the `(None, 8, 8, 2048)` tensor.\n",
    "\n",
    "* We'll then have a new deep network that reuses the standard Inception-v3 network, but it is trained on a new domain D via transfer learning;\n",
    "* Even though there are many parameters to fine-tune for achieving good accuracy, we are now reusing a very large pretrained network as a starting point via transfer learning;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Create the base pre-trained model\n",
    "# We don't include the top model because we want to finetune on D\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# Adds a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # let's add a fully-connected layer as first layer\n",
    "x = Dense(1024, activation='relu')(x) # and a logistic layer with 200 classes as last layer\n",
    "predictions = Dense(200, activation='softmax')(x) # model to train\n",
    "model = Model(input=base_model.input, output=predictions)\n",
    "\n",
    "# All the convolutional levels are pre-trained, so we freeze them during the training of the full model\n",
    "for layer in base_model.layers: layer.trainable = False\n",
    "\n",
    "# compile the model (should be done after setting layers to nontrainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "# The model is then compiled and trained for a few epochs so that the top layers are trained\n",
    "# Train the model on the new data for a few epochs\n",
    "model.fit_generator(...)\n",
    "\n",
    "# Then we freeze the top layers in inception and fine-tune some inception layer\n",
    "# In this example, we decide to freeze the first 172 layers (an hyperparameter to tune)\n",
    "for layer in model.layers[:172]: layer.trainable = False\n",
    "for layer in model.layers[172:]: layer.trainable = True\n",
    "\n",
    "# The model is then recompiled for fine-tune optimization. We use SGD with a low learning rate\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# We train our model again (this time fine-tuning the top 2 inception blocks) alongside the top Dense layers\n",
    "model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative adversarial  networks (GAN) and WaveNet\n",
    "\n",
    "* GANs have been defined as the most interesting idea in the last 10 years of Machine Learning;\n",
    "* The key intuition of GAN can be easily considered as analogous to art forgery, which is the process of creating works of art;\n",
    "* GANs are able to learn how to reproduce synthetic data that looks real;\n",
    "* Computers can learn how to paint and create realistic images;\n",
    "* _WaveNet_ is a deep generative network proposed by Google DeepMind to teach computers how to reproduce human voices and musical instruments, both with impressive quality;\n",
    "* GANs train two neural nets simultaneously, as shown in the next diagram:\n",
    "  - The generator G(Z) makes the forgery;\n",
    "  - The discriminator D(Y) can judge how realistic the reproductions are;\n",
    "* G(Z) takes an input from a random noise, Z, and trains itself to fool D into thinking that whatever G(Z) produces is real;\n",
    "* So, G and D play an opposite game; hence the name adversarial training and their objectives is expressed as a loss function optimized via a gradient descent;\n",
    "* The generative model learns how to forge more successfully, and the discriminative model learns how to recognize forgery more successfully.\n",
    "* At the end, the generator will learn how to produce forged images that are indistinguishable from real ones;\n",
    "* GANs require finding the equilibrium in a game with two players;\n",
    "* Sometimes the two players eventually reach an equilibrium, but this is not always guaranteed and the two players can continue playing for a long time;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "* It's a way to transform words in text to numerical vectors so that they can be analyzed by machine learning algorithms;\n",
    "* _One hot encoding_ is the most basic embedding apprach. It represents a word by a vector of the size of the vocabulary, where only the entry corresponding to that word is 1 and all the others are 0;\n",
    "* The main problem with _one hot encoding_ is that there's no way to represent the similarity between words;\n",
    "* Similarity between vectors is computed using the dot product, so the dot product between two words is always zero;\n",
    "* The NLP community has borrowed techniques such as TF-IDF, _latent semantic analysis (LSA)_ and topic modeling to use the documents as the context;\n",
    "* However, these representations capture a slightly different document-centric idea of semantic similarity;\n",
    "* Today, word embedding is the technique of choice for vectorizing text for all kinds of NLP tasks, such as text classification, document clustering, part of speech tagging, named entity recognition, sentiment analysis and so on;\n",
    "\n",
    "### Distributed representations\n",
    "\n",
    "* _Distributed representations_ attempt to capture the meaning of a word by considering its relations with other words in its context;\n",
    "\n",
    "For example, consider the following pair of sentences:\n",
    "\n",
    "1. _Paris is the capital of France_\n",
    "2. _Berlin is the capital of Germany_\n",
    "\n",
    "You would still conclude without too much effort that the word pairs `(Paris, Berlin)` and `(France, Germany)` are related in some way:\n",
    "\n",
    "`Paris : France :: Berlin : Germany`\n",
    "\n",
    "Thus, the aim of distributed representations is to find a general transformation function φ to convert each word to its associated vector such that relations of the following form hold true:\n",
    "\n",
    "`φ(\"Paris\") - φ(\"France\") ≈ φ(\"Berlin\") - φ(\"Germany\")`\n",
    "\n",
    "### word2vec\n",
    "\n",
    "* Created in 2003 at Google;\n",
    "* The models are unsupervised, taking as input a large corpus of text and producing a vector space of words;\n",
    "* The dimensionality of the word2vec embedding space is usually lower than the dimensionality of the one-hot embedding space;\n",
    "* It has 2 architectures:\n",
    "  - Continuous bag of words (CBOW);\n",
    "  - Skip-gram;\n",
    "* In CBOW, the model predicts the current word given a window of surrounding words;\n",
    "* In the Skip-gram arquitecture, the model predicts the surroundig words given the center word;\n",
    "* According to authors, CBOW is faster, but Skip-gram does a better job at predicting _infrequent words_;\n",
    "* It's interesting to note that both flavors of word2vec are shallow neural networks;\n",
    "\n",
    "#### Skip-gram word2vec model\n",
    "\n",
    "* The skip-gram model is trained to predict the surrounding words given the current word;\n",
    "\n",
    "Consider this example:\n",
    "\n",
    "`I love green eggs and ham.`\n",
    "\n",
    "Assuming a window size of three, we can break it in the following set of `(context, word)` pairs:\n",
    "\n",
    "```\n",
    "([I, green], love)\n",
    "([love, eggs], green)\n",
    "([green, and], eggs)\n",
    "([eggs, ham], and)\n",
    "```\n",
    "\n",
    "* Since the skip-gram model predicts a context word given the center word, we can convert the preceding dataset to one of (input, output) pairs;\n",
    "* We then generate positive examples by combining correct predictions with a result of 1 and negative examples by combining random words with a result of 0:\n",
    "\n",
    "```\n",
    "((love, I), 1)\n",
    "((love, green), 1)\n",
    "...\n",
    "((love, ham), 0)\n",
    "((love, and), 0)\n",
    "```\n",
    "\n",
    "* We can now train a classifier that takes in a word vector and a context vector and learns to predict one or zero depending on whether it sees a positive or negative sample;\n",
    "* The deliverables from this trained network are the weights of the word embedding layer;\n",
    "* The skip-gram model can be built in Keras as follows. Assume that the vocabulary size is set at 5000, the output embedding size is 300 and the window size is 1 (a window size of one means that the context for a word is the words immediately to the left and right);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dot\n",
    "from keras.layers.core import Dense, Reshape\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.engine.input_layer import Input\n",
    "\n",
    "vocab_size = 5000\n",
    "embed_size = 300\n",
    "\n",
    "# The input to this model is the word ID in the vocabulary\n",
    "# The embedding weights are initially set to small random values\n",
    "# The next layer reshapes the input to the embedding size\n",
    "word_model = Sequential()\n",
    "word_model.add(Embedding(vocab_size, embed_size, embeddings_initializer=\"glorot_uniform\", input_length=1))\n",
    "word_model.add(Reshape((embed_size, )))\n",
    "\n",
    "# The other model that we need is a sequential model for the context words\n",
    "# For each of our skip-gram pairs, we have a single context word corresponding to the target word\n",
    "context_model = Sequential()\n",
    "context_model.add(Embedding(vocab_size, embed_size, embeddings_initializer=\"glorot_uniform\", input_length=1))\n",
    "context_model.add(Reshape((embed_size,)))\n",
    "\n",
    "# The outputs of the two models are each a vector of size (embed_size).They're both merged into one\n",
    "# using a dot product and fed into a dense layer.\n",
    "# The sigmoid activation function modulates the output so numbers higher than 0.5 tend rapidly to 1 and\n",
    "# flatten out.\n",
    "merged_output = dot([word_model.output, context_model.output], axes=1)\n",
    "dot_product_output = Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")(merged_output)\n",
    "model = Model([word_model.input, context_model.input], dot_product_output)\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "\n",
    "# The loss function used is the mean_squared_error. The idea is to minimize the dot product for positive\n",
    "# examples and maximize it for negative examples. The dot product multiplies corresponding elements of vectors\n",
    "# and sums up the result, which causes similar vectors to have higher dot products than dissimilar vectors,\n",
    "# since the former has more overlapping elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Keras provides a convenience function to extract skip-grams for a text that has been converted to a list of word indices;\n",
    "* This is an example of using this function to extract the first 10 of 56 skip-grams generated (both positive and negative);\n",
    "* The tokenizer creates a dictionary mapping each unique word to an integer ID and makes it available in the `word_index` attribute;\n",
    "* The `skip-gram` method randomly samples the results from the pool of possibilities for the positive examples;\n",
    "* The process of negative sampling, used for generating the negative examples, consists of randomly pairing up arbitrary tokens from the text. As the size of the input increases, it is more likely to pick up unrelated word pairs, but in this small example it can end pu generating positive examples as well;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "#text = \"I love green eggs and ham .\"\n",
    "text = \"My life has been getting more and more complicated as the size of the input has increased with time.\"\n",
    "\n",
    "# Declare the tokenizer and run the text against it.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# Extracts the {word: id} dictionary and create a two-way lookup table:\n",
    "word2id = tokenizer.word_index # {'i': 1, 'love': 2, 'eggs': 4, 'and': 5, 'ham': 6}\n",
    "id2word = { v:k for k, v in word2id.items() } # {1: 'i', 2: 'love', 3: 'green', 4: 'eggs', 5: 'and', 6: 'ham'}\n",
    "\n",
    "# Convert our input list of words to a list of IDs and pass it to the skipgrams function.\n",
    "wids = [word2id[w] for w in text_to_word_sequence(text)] # [1, 2, 3, 4, 5, 6]\n",
    "pairs, labels = skipgrams(wids, len(word2id))\n",
    "#print('pairs', pairs) # [[6, 4], [6, 5], [5, 3], ..., [3, 5], [1, 3]]\n",
    "#print('labels', labels) # [0, 1, 0, ..., 0, 1]\n",
    "\n",
    "# Prints the first 10 words from the pool of possibilities\n",
    "for i in range(10):\n",
    "    print(\"(({:s} ({:d}), {:s} ({:d})), {:d})\".format(\n",
    "        id2word[pairs[i][0]], pairs[i][0],\n",
    "        id2word[pairs[i][1]], pairs[i][1],\n",
    "        labels[i])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW word2vec model\n",
    "\n",
    "* The CBOW model predicts the center word given the context words;\n",
    "* In the first tuple in the following example, the CBOW model needs to predict the output word `love`, given the context words `I` and `green`;\n",
    "\n",
    "```\n",
    "[I, green], love)\n",
    "([love, eggs], green)\n",
    "([green, and], eggs)\n",
    "...\n",
    "```\n",
    "\n",
    "* Like the skip-gram model, the CBOW model is also a classifier that takes the context words as input and predicts the target word;\n",
    "* The input to the model is the word IDs for the context words;\n",
    "* These word IDs are fed into a common embedding layer that is initialized with small random weights;\n",
    "* Each word ID is transformed into a vector of size `(embed_size)` by the embedding layer;\n",
    "* Thus, each row of the input context is transformed into a matrix of size (2 * window_size, embed_size) by this layer;\n",
    "* This is then fed into a lambda layer, which computes an average of all the embeddings;\n",
    "* This average is then fed to a dense layer, which creates a dense vector of size `(vocab_size)` for each row;\n",
    "* The activation function on the dense layer is a `softmax`, which reports the maximum value on the output vector as a probability;\n",
    "* The ID with the maximum probability corresponds to the target word;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras.backend as K\n",
    "\n",
    "vocab_size = 5000\n",
    "embed_size = 300\n",
    "window_size = 1\n",
    "\n",
    "# Note that the input_length of this embedding layer is equal to the number of context words.\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embed_size,\n",
    "        embeddings_initializer='glorot_uniform',\n",
    "        input_length=window_size*2\n",
    "    )\n",
    ")\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "model.add(Dense(vocab_size, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "\n",
    "# The loss function used here is categorical_crossentropy, which is a common choice for cases where there\n",
    "# are two or more (in our case, vocab_size) categories.\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting word2vec embeddings from the model\n",
    "\n",
    "* Although word2vec models are classification problems, we are more interested in the side effect of this classification process;\n",
    "* There are many examples of how these distributed representations exhibit often surprising syntactic and semantic information;\n",
    "* Vectors connecting words that have similar meanings but opposite genders are approximately parallel in the reduced 2D space, and we can often get very intuitive results by doing arithmetic with the word vectors;\n",
    "* Intuitively, the training process imparts enough information to the internal encoding to predict an output word that occurs in the context of an input word;\n",
    "\n",
    "Keras provides a way to extract weights from trained models. For the skip-gram example, the embedding weights can be extracted as follows:\n",
    "\n",
    "```\n",
    "merge_layer = model.layers[0]\n",
    "word_model = merge_layer.layers[0]\n",
    "word_embed_layer = word_model.layers[0]\n",
    "weights = word_embed_layer.get_weights()[0]\n",
    "```\n",
    "\n",
    "Similarly, the embedding weights for the CBOW example can be extracted using the following one-liner:\n",
    "\n",
    "```\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "```\n",
    "\n",
    "* In both cases, the shape of the weights matrix is `vocab_size` and `embed_size`;\n",
    "* In order to compute the distributed representation for a word in the vocabulary, you will need to construct a one-hot vector by setting the position of the word index to one in a zero vector of size `(vocab_size)` and multiply it with the matrix to get the embedding vector of size `(embed_size)`.\n",
    "\n",
    "### Use 3rd-party implementations or word2vec\n",
    "\n",
    "* Although you can implement word2vec models on your own, third-party implementations are readily available, and unless your use case is very complex or different, it makes sense to just use one such implementation instead of rolling your own;\n",
    "* The `gensim` library provides an implementation of word2vec.\n",
    "* Sice Keras does not provide any support for word2vec, integrating the `gensim` implementation into Keras code is very common practice;\n",
    "* The following code shows how to build a word2vec model using `gensim` and train it with the text from the `text8` corpus (available for download at https://matthoney.net/dc/text8.zip) which is a file containing about 17 million words derived from Wikipedia. Wikipedia text was cleaned to remove markup, punctuation, and non-ASCII text, and the first 100 million characters of this cleaned text became the text8 corpus. This corpus is commonly used as an example for word2vec because it is quick to train and produces good results.\n",
    "\n",
    "The steps go as follows:\n",
    "\n",
    "* We read in the words from the text8 corpus, and split up the words into sentences of 50 words each (the `gensim` library provides a built-in text8 handler that does something similar);\n",
    "* Since we want to illustrate how to generate a model with any (preferably large) corpus that may or may not fit into memory, we will show you how to generate these sentences using a Python generator;\n",
    "* The Text8Sentences class will generate sentences of maxlen words each from the text8 file;\n",
    "* In this case, we do ingest the entire file into memory, but when traversing through directories of files, generators allows us to load parts of the data into memory at a time, process them, and yield them to the caller;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import logging\n",
    "import os\n",
    "\n",
    "class Text8Sentences(object):\n",
    "    \n",
    "    def __init__(self, fname, maxlen):\n",
    "        self.fname = fname\n",
    "        self.maxlen = maxlen\n",
    "    \n",
    "    def __iter__(self):\n",
    "        with open(os.path.join(DATA_DIR, \"text8\"), \"rb\") as ftext:\n",
    "            text = ftext.read().split(\" \")\n",
    "            sentences, words = [], []\n",
    "            for word in text:\n",
    "                if len(words) >= self.maxlen:\n",
    "                    yield words\n",
    "                    words = []\n",
    "                    words.append(word)\n",
    "                    yield words\n",
    "\n",
    "#The gensim word2vec uses Python logging to report on progress, so we first enable it.\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "# Declares an instance of the Text8Sentences class, and the line after that trains the model with\n",
    "# the sentences from the dataset.\n",
    "sentences = Text8Sentences(os.path.join(DATA_DIR, \"text8\"), 50)\n",
    "\n",
    "# We have chosen the size of the embedding vectors to be 300 and we only consider words that appear\n",
    "# a minimum of 30 times in the corpus.\n",
    "# The default window size is 5, so we will consider 5 words before and after the current word.\n",
    "# By default, the word2vec model created is CBOW, but you can change that by setting sg=1 in the parameters.\n",
    "model = word2vec.Word2Vec(sentences, size=300, min_count=30)\n",
    "\n",
    "# The word2vec implementation will make two passes over the data, first to generate a vocabulary and then\n",
    "# to build the actual model.\n",
    "\n",
    "# Once the model is created, we should normalize the resulting vectors. According to the documentation,\n",
    "# this saves lots of memory. Once the model is trained, we can optionally save it to disk:\n",
    "model.init_sims(replace=True)\n",
    "model.save(\"word2vec_gensim.bin\")\n",
    "\n",
    "# The model can be brought back into memory using the following call:\n",
    "model = Word2Vec.load(\"word2vec_gensim.bin\")\n",
    "\n",
    "# We can now query the model to find all the words it knows about:\n",
    "model.vocab.keys()[0:4] # ['homomorphism', 'woods', 'spiders', 'hanging']\n",
    "\n",
    "# We can find the actual vector embedding for a given word:\n",
    "model[\"woman\"] # array([ -3.13099056e-01, -1.85702944e+00, ..., -1.30940580e+00], dtype=”float32”)\n",
    "\n",
    "# We can also find words that are most similar to a certain word:\n",
    "model.most_similar(\"woman\") # [('child', 0.706), ('girl', 0.702), ..., ('daughter', 0.587)]\n",
    "\n",
    "# We can provide hints for finding word similarity. For example, the following command returns the\n",
    "# top 10 words that are like woman and king but unlike man:\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10) # [('queen', 0.624), ('prince', 0.564), ..., ('matilda', 0.517)]\n",
    "\n",
    "# We can also find similarities between individual words:\n",
    "model.similarity(\"girl\", \"woman\") # 0.702182479574"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
